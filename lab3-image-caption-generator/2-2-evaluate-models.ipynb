{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from tensorflow.python.keras.models import load_model\n",
    "from tensorflow.python.client import device_lib\n",
    "from keras_preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.merge import add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "curr_folder = \"D:/YandexDisk/datasets/\"\n",
    "\n",
    "path_features = curr_folder + \"ru-12k-features.pkl\"\n",
    "path_tokenizer = curr_folder + \"ru-12k-tokenizer-train.pkl\"\n",
    "\n",
    "path_train_dict = curr_folder + \"captions-ru-12k-train.pkl\"\n",
    "path_val_dict = curr_folder + \"captions-ru-12k-val.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def load_image_features(filename, data):\n",
    "    all_features = pickle.load(open(filename, 'rb'))\n",
    "    features = {k: all_features[k] for k in data}\n",
    "\n",
    "    return features\n",
    "\n",
    "def to_lines(data):\n",
    "    all_vals = list()\n",
    "    for key in data.keys():\n",
    "        [all_vals.append(d) for d in data[key]]\n",
    "\n",
    "    return all_vals\n",
    "\n",
    "def create_tokenizer(data):\n",
    "    lines = to_lines(data)\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "def find_max_words(data):\n",
    "    lines = to_lines(data)\n",
    "    return max(len(l.split()) for l in lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Наборы для обучения и валидации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "кол-во подписей .............. 8262\n",
      "размер словаря ............... 21391\n",
      "длина предложения в словах ... 22\n"
     ]
    }
   ],
   "source": [
    "with open (path_train_dict, 'rb') as f:\n",
    "    train_dict = pickle.load(f)\n",
    "print('кол-во подписей .............. %d' % len(train_dict))\n",
    "\n",
    "with open (path_tokenizer, 'rb') as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('размер словаря ............... %d' % vocab_size)\n",
    "\n",
    "max_words = find_max_words(train_dict)\n",
    "print('длина предложения в словах ... %d' % max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "кол-во подписей .............. 2360\n",
      "размер словаря ............... 10450\n",
      "длина предложения в словах ... 21\n"
     ]
    }
   ],
   "source": [
    "with open (path_val_dict, 'rb') as f:\n",
    "    val_dict = pickle.load(f)\n",
    "val_features = load_image_features(path_features, val_dict)\n",
    "print('кол-во подписей .............. %d' % len(val_dict))\n",
    "\n",
    "val_tokenizer = create_tokenizer(val_dict)\n",
    "val_vocab_size = len(val_tokenizer.word_index) + 1\n",
    "print('размер словаря ............... %d' % val_vocab_size)\n",
    "\n",
    "val_max_words = find_max_words(val_dict)\n",
    "print('длина предложения в словах ... %d' % val_max_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Оценка модели\n",
    "\n",
    "После того, как модель обучены, мы можем оценить качество её предсказаний в тестовом наборе данных.\n",
    "\n",
    "Мы оценим модель, создав описания для всех изображений в тестовом наборе данных и оценив эти прогнозы с помощью стандартной функции затрат.\n",
    "\n",
    "Во-первых, нам нужно иметь возможность сгенерировать описание изображения, используя обученную модель. Это включает в себя передачу маркера начального описания \"startseq\", генерацию одного слова, а затем рекурсивный вызов модели с сгенерированными словами в качестве входных данных до тех пор, пока не будет достигнут конец маркера последовательности \"endseq\" или не будет достигнута максимальная длина описания.\n",
    "\n",
    "Приведённая ниже функция реализует это поведение и генерирует текстовое описание с учётом обученной модели и заданного подготовленного изображения в качестве входных данных. Эта функция вызывает другую функцию map_int_to_word, чтобы отобразить целочисленное предсказание обратно в слово."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def map_int_to_word(integer, tokenizer):\n",
    "    for word, idx in tokenizer.word_index.items():\n",
    "        if idx == integer:\n",
    "            return word\n",
    "\n",
    "    return None\n",
    "\n",
    "# 0 seed the generation process\n",
    "# 1 iterate over the whole length of the sequence\n",
    "# 2 integer encode input sequence\n",
    "# 3 pad input\n",
    "# 4 predict next word\n",
    "# 5 convert probability to integer\n",
    "# 6 map integer to word\n",
    "# 7 stop if we cannot map the word\n",
    "# 8 append as input for generating the next word\n",
    "# 9 stop if we predict the end of the sequence\n",
    "def generate_caption(model, tokenizer, image, max_words):\n",
    "    in_text = 'startseq'                                    # 0\n",
    "\n",
    "    for i in range(max_words):                              # 1\n",
    "        seq = tokenizer.texts_to_sequences([in_text])[0]    # 2\n",
    "        seq = pad_sequences([seq], maxlen=max_words)        # 3\n",
    "\n",
    "        y_hat = model.predict([image,seq], verbose=0)       # 4\n",
    "        y_hat = np.argmax(y_hat)                            # 5\n",
    "\n",
    "        word = map_int_to_word(y_hat, tokenizer)            # 6\n",
    "        if word is None:                                    # 7\n",
    "            break\n",
    "\n",
    "        in_text += ' ' + word                               # 8\n",
    "\n",
    "        if word == 'endseq':                                # 9\n",
    "            break\n",
    "\n",
    "    return in_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### BLEU\n",
    "\n",
    "Мы сгенерируем предсказания для всех изображений в тестовом наборе данных и в наборе данных для обучения.\n",
    "\n",
    "Приведенная ниже функция с именем evaluate_model() будет оценивать обученную модель по заданному набору подписей изображений и признаков изображений. Фактические и прогнозируемые описания собираются и оцениваются вместе с использованием оценки алгоритма BLEU, который оценивает, насколько сгенерированный текст близок к ожидаемому тексту.\n",
    "\n",
    "BLEU используется при переводе текста для оценки переведённого текста по одному или нескольким другим переводам.\n",
    "\n",
    "Here, we compare each generated description against all of the reference descriptions for the photograph. We then calculate BLEU scores for 1, 2, 3 and 4 cumulative n-grams.\n",
    "\n",
    "The NLTK Python library implements the BLEU score calculation in the corpus_bleu() function. A higher score close to 1.0 is better, a score closer to zero is worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, captions, images, tokenizer, max_words):\n",
    "    actual, predicted = list(), list()\n",
    "\n",
    "    for key, captions_list in captions.items():\n",
    "        references = [c.split() for c in captions_list]\n",
    "        y_hat = generate_caption(model, tokenizer, images[key], max_words)\n",
    "\n",
    "        actual.append(references)\n",
    "        predicted.append(y_hat.split())\n",
    "\n",
    "    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "    print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "    print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
    "    print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Оценка VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.361409\n",
      "BLEU-2: 0.193161\n",
      "BLEU-3: 0.137490\n",
      "BLEU-4: 0.055897\n"
     ]
    }
   ],
   "source": [
    "path_model = 'D:/models/vgg16/model-0.h5'\n",
    "model = load_model(path_model)\n",
    "evaluate_model(model, val_dict, val_features, tokenizer, max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.406490\n",
      "BLEU-2: 0.224477\n",
      "BLEU-3: 0.165109\n",
      "BLEU-4: 0.074745\n"
     ]
    }
   ],
   "source": [
    "path_model = 'D:/models/vgg16/model-1.h5'\n",
    "model = load_model(path_model)\n",
    "evaluate_model(model, val_dict, val_features, tokenizer, max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.468829\n",
      "BLEU-2: 0.259854\n",
      "BLEU-3: 0.191766\n",
      "BLEU-4: 0.092175\n"
     ]
    }
   ],
   "source": [
    "path_model = 'D:/models/vgg16/model-2.h5'\n",
    "model = load_model(path_model)\n",
    "evaluate_model(model, val_dict, val_features, tokenizer, max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.512229\n",
      "BLEU-2: 0.289319\n",
      "BLEU-3: 0.215655\n",
      "BLEU-4: 0.107287\n"
     ]
    }
   ],
   "source": [
    "path_model = 'D:/models/vgg16/model-3.h5'\n",
    "model = load_model(path_model)\n",
    "evaluate_model(model, val_dict, val_features, tokenizer, max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.501154\n",
      "BLEU-2: 0.279348\n",
      "BLEU-3: 0.207381\n",
      "BLEU-4: 0.103278\n"
     ]
    }
   ],
   "source": [
    "path_model = 'D:/models/vgg16/model-4.h5'\n",
    "model = load_model(path_model)\n",
    "evaluate_model(model, val_dict, val_features, tokenizer, max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.501107\n",
      "BLEU-2: 0.275949\n",
      "BLEU-3: 0.202924\n",
      "BLEU-4: 0.100838\n"
     ]
    }
   ],
   "source": [
    "path_model = 'D:/models/vgg16/model-5.h5'\n",
    "model = load_model(path_model)\n",
    "evaluate_model(model, val_dict, val_features, tokenizer, max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.494001\n",
      "BLEU-2: 0.268168\n",
      "BLEU-3: 0.196033\n",
      "BLEU-4: 0.095993\n"
     ]
    }
   ],
   "source": [
    "path_model = 'D:/models/vgg16/model-6.h5'\n",
    "model = load_model(path_model)\n",
    "evaluate_model(model, val_dict, val_features, tokenizer, max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.489138\n",
      "BLEU-2: 0.265169\n",
      "BLEU-3: 0.192033\n",
      "BLEU-4: 0.093191\n"
     ]
    }
   ],
   "source": [
    "path_model = 'D:/models/vgg16/model-7.h5'\n",
    "model = load_model(path_model)\n",
    "evaluate_model(model, val_dict, val_features, tokenizer, max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.487587\n",
      "BLEU-2: 0.264134\n",
      "BLEU-3: 0.191545\n",
      "BLEU-4: 0.091213\n"
     ]
    }
   ],
   "source": [
    "path_model = 'D:/models/vgg16/model-8.h5'\n",
    "model = load_model(path_model)\n",
    "evaluate_model(model, val_dict, val_features, tokenizer, max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.482318\n",
      "BLEU-2: 0.260322\n",
      "BLEU-3: 0.188676\n",
      "BLEU-4: 0.088879\n"
     ]
    }
   ],
   "source": [
    "path_model = 'D:/models/vgg16/model-9.h5'\n",
    "model = load_model(path_model)\n",
    "evaluate_model(model, val_dict, val_features, tokenizer, max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.477577\n",
      "BLEU-2: 0.255085\n",
      "BLEU-3: 0.185947\n",
      "BLEU-4: 0.088144\n"
     ]
    }
   ],
   "source": [
    "path_model = 'D:/models/vgg16/model-10.h5'\n",
    "model = load_model(path_model)\n",
    "evaluate_model(model, val_dict, val_features, tokenizer, max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.476834\n",
      "BLEU-2: 0.254644\n",
      "BLEU-3: 0.186213\n",
      "BLEU-4: 0.089660\n"
     ]
    }
   ],
   "source": [
    "path_model = 'D:/models/vgg16/model-11.h5'\n",
    "model = load_model(path_model)\n",
    "evaluate_model(model, val_dict, val_features, tokenizer, max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.473901\n",
      "BLEU-2: 0.252494\n",
      "BLEU-3: 0.183874\n",
      "BLEU-4: 0.087916\n",
      "Wall time: 33min\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "path_model = 'D:/models/vgg16/model-12.h5'\n",
    "model = load_model(path_model)\n",
    "evaluate_model(model, val_dict, val_features, tokenizer, max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.472866\n",
      "BLEU-2: 0.252232\n",
      "BLEU-3: 0.183842\n",
      "BLEU-4: 0.086977\n",
      "Wall time: 37min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "path_model = 'D:/models/vgg16/model-13.h5'\n",
    "model = load_model(path_model)\n",
    "evaluate_model(model, val_dict, val_features, tokenizer, max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.464693\n",
      "BLEU-2: 0.247082\n",
      "BLEU-3: 0.180864\n",
      "BLEU-4: 0.086513\n"
     ]
    }
   ],
   "source": [
    "path_model = 'D:/models/model-14.h5'\n",
    "model = load_model(path_model)\n",
    "evaluate_model(model, val_dict, val_features, tokenizer, max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.463840\n",
      "BLEU-2: 0.247353\n",
      "BLEU-3: 0.181503\n",
      "BLEU-4: 0.088804\n",
      "Wall time: 32min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "path_model = 'D:/models/vgg16/model-15.h5'\n",
    "model = load_model(path_model)\n",
    "evaluate_model(model, val_dict, val_features, tokenizer, max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.472879\n",
      "BLEU-2: 0.253866\n",
      "BLEU-3: 0.186495\n",
      "BLEU-4: 0.091238\n",
      "Wall time: 33min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "path_model = 'D:/models/vgg16/model-16.h5'\n",
    "model = load_model(path_model)\n",
    "evaluate_model(model, val_dict, val_features, tokenizer, max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.461818\n",
      "BLEU-2: 0.244349\n",
      "BLEU-3: 0.178495\n",
      "BLEU-4: 0.085330\n",
      "Wall time: 32min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "path_model = 'D:/models/vgg16/model-17.h5'\n",
    "model = load_model(path_model)\n",
    "evaluate_model(model, val_dict, val_features, tokenizer, max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.459094\n",
      "BLEU-2: 0.242283\n",
      "BLEU-3: 0.176489\n",
      "BLEU-4: 0.083362\n",
      "Wall time: 31min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "path_model = 'D:/models/vgg16/model-18.h5'\n",
    "model = load_model(path_model)\n",
    "evaluate_model(model, val_dict, val_features, tokenizer, max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.464013\n",
      "BLEU-2: 0.243522\n",
      "BLEU-3: 0.175339\n",
      "BLEU-4: 0.082336\n"
     ]
    }
   ],
   "source": [
    "path_model = 'D:/models/model-19.h5'\n",
    "model = load_model(path_model)\n",
    "evaluate_model(model, val_dict, val_features, tokenizer, max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.446779\n",
      "BLEU-2: 0.227613\n",
      "BLEU-3: 0.163310\n",
      "BLEU-4: 0.073715\n"
     ]
    }
   ],
   "source": [
    "path_model = 'D:/models/model-39.h5'\n",
    "model = load_model(path_model)\n",
    "evaluate_model(model, val_dict, val_features, tokenizer, max_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}