{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "from keras_preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.merge import add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 13496150933907984846\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 3129068339\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 12886038549300639828\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "curr_folder = \"D:/YandexDisk/datasets/\"\n",
    "\n",
    "start_dir = \"D:/datasets/flickr-images-30k\"\n",
    "end_dir = \"D:/datasets/flickr-images-12k\"\n",
    "\n",
    "path_captions = curr_folder + \"captions-ru-12k.csv\"\n",
    "path_captions_no_puncts = curr_folder + \"captions-ru-12k-no-puncts.csv\"\n",
    "path_train = curr_folder + \"captions-ru-12k-train.csv\"\n",
    "path_val = curr_folder + \"captions-ru-12k-val.csv\"\n",
    "path_test = curr_folder + \"captions-ru-12k-test.csv\"\n",
    "\n",
    "path_features = curr_folder + \"ru-12k-features.pkl\"\n",
    "path_vocab = curr_folder + \"ru-12k-vocab.pkl\"\n",
    "path_sentences = curr_folder + \"ru-12k-sentences-train.pkl\"\n",
    "\n",
    "path_train_dict = curr_folder + \"captions-ru-12k-train.pkl\"\n",
    "\n",
    "path_model = curr_folder + \"model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5\"\n",
    "\n",
    "def image_names_set(data):\n",
    "    vals = set()\n",
    "\n",
    "    for idx in data.index:\n",
    "        vals.add(data.iat[idx, 0][:-4])\n",
    "\n",
    "    return vals\n",
    "\n",
    "def load_image_features(filename, data):\n",
    "    all_features = pickle.load(open(filename, 'rb'))\n",
    "    features = {k: all_features[k] for k in data}\n",
    "\n",
    "    return features\n",
    "\n",
    "def to_lines(data):\n",
    "    all_vals = list()\n",
    "    for key in data.keys():\n",
    "        [all_vals.append(d) for d in data[key]]\n",
    "\n",
    "    return all_vals\n",
    "\n",
    "def create_tokenizer(data):\n",
    "    lines = to_lines(data)\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "def find_max_words(data):\n",
    "    lines = to_lines(data)\n",
    "    return max(len(l.split()) for l in lines)\n",
    "\n",
    "def data_generator(tokenizer, max_length, data, images, batch_size, random_seed):\n",
    "    count = 0\n",
    "    random.seed(random_seed)\n",
    "\n",
    "    img_names = list(data.keys())\n",
    "    assert batch_size <= len(img_names), 'batch size must be less than or equal to {}'.format(len(img_names))\n",
    "\n",
    "    while True:\n",
    "        input_img_batch, input_seq_batch, output_word_batch = list(), list(), list()\n",
    "\n",
    "        if count >= len(img_names):\n",
    "            count = 0\n",
    "        start_i = count\n",
    "        end_i = min(len(img_names), count + batch_size)\n",
    "\n",
    "        for i in range(start_i, end_i):\n",
    "            curr_img = img_names[i]\n",
    "            image = images[curr_img][0]\n",
    "            captions_list = data[curr_img]\n",
    "            random.shuffle(captions_list)\n",
    "\n",
    "            input_img, input_seq, output_word = create_sequences(tokenizer, max_length, captions_list, image)\n",
    "\n",
    "            for j in range(len(input_img)):\n",
    "                input_img_batch.append(input_img[j])\n",
    "                input_seq_batch.append(input_seq[j])\n",
    "                output_word_batch.append(output_word[j])\n",
    "\n",
    "        count = count + batch_size\n",
    "        yield [np.array(input_img_batch), np.array(input_seq_batch)], np.array(output_word_batch)\n",
    "\n",
    "def create_sequences(tokenizer, max_length, captions_list, image_name):\n",
    "    X_image, X_text, y_word = list(), list(), list()\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "    for caption in captions_list:\n",
    "        seq = tokenizer.texts_to_sequences([caption])[0]\n",
    "\n",
    "        for i in range(1, len(seq)):\n",
    "            in_seq, out_seq = seq[:i], seq[i]\n",
    "            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "\n",
    "            X_image.append(image_name)\n",
    "            X_text.append(in_seq)\n",
    "            y_word.append(out_seq)\n",
    "\n",
    "    return X_image, X_text, y_word\n",
    "\n",
    "def build_model(vocab_size, max_length):\n",
    "    inputs1 = Input(shape=(4096,))\n",
    "    fe1 = Dropout(0.5)(inputs1)\n",
    "    fe2 = Dense(256, activation='relu')(fe1)\n",
    "\n",
    "    inputs2 = Input(shape=(max_length,))\n",
    "    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "    se2 = Dropout(0.5)(se1)\n",
    "    se3 = LSTM(256)(se2)\n",
    "\n",
    "    de1 = add([fe2, se3])\n",
    "    de2 = Dense(256, activation='relu')(de1)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(de2)\n",
    "\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загрузка данных\n",
    "\n",
    "Мы собираемся обучить данные по всем фотографиям и подписям в обучающем наборе данных. Во время обучения мы будем отслеживать производительность модели в наборе данных разработки и использовать эту производительность, чтобы решить, когда сохранять модели в файл.\n",
    "\n",
    "Модель, которую мы разработаем, будет генерировать подпись к фотографии, и подпись будет генерироваться по одному слову за раз.\n",
    "\n",
    "Последовательность ранее сгенерированных слов будет предоставлена в качестве входных данных. Поэтому нам понадобится \"первое слово\", чтобы начать процесс генерации, и \"последнее слово\", чтобы сигнализировать об окончании подписи. Для этой цели мы будем использовать строки \"startseq\" и \"endseq\". Эти маркеры добавляются к загруженным описаниям по мере их загрузки. Важно сделать это сейчас, прежде чем мы закодируем текст, чтобы токены также были закодированы правильно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Закодировать знаки в числа\n",
    "\n",
    "Текст описания необходимо будет закодировать в числа, прежде чем его можно будет представить модели в качестве входных данных или сравнить с предсказаниями модели.\n",
    "\n",
    "Первым шагом в кодировании данных является создание согласованного сопоставления слов с уникальными целочисленными значениями. Keras предоставляет класс Tokenizer, который может изучить это сопоставление из загруженных данных описания.\n",
    "\n",
    "Каждое описание будет разделено на слова. Модель будет предоставлена одним словом и фотографией и сгенерирует следующее слово. Затем первые два слова описания будут предоставлены модели в качестве входных данных вместе с изображением для создания следующего слова. Именно так будет обучаться модель."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Создание последовательности\n",
    "\n",
    "Приведенная ниже функция с именем create_sequences(), учитывая токенизатор, максимальную длину последовательности и словарь всех описаний и фотографий, преобразует данные в пары ввода-вывода данных для обучения модели.\n",
    "\n",
    "В модели есть два входных массива: один для признаков фотографии и один для закодированного текста. Существует один вывод для модели, который представляет собой закодированное следующее слово в текстовой последовательности.\n",
    "\n",
    "Входной текст кодируется в виде целых чисел, которые будут передаваться на слой встраивания слов. Признаки изображения будут передаваться непосредственно в другую часть модели. Модель выдаст прогноз, который будет представлять собой распределение вероятностей по всем словам в словаре.\n",
    "\n",
    "Таким образом, выходные данные будут представлять собой однократно закодированную версию каждого слова, представляющую идеализированное распределение вероятностей со значениями 0 во всех позициях слов, кроме фактической позиции слова, которая имеет значение 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Генератор данных\n",
    "\n",
    "Генератор данных будет выдавать данные на одно изображении в каждой партии. Это будут все последовательности, сгенерированные для изображения и её набора описаний.\n",
    "\n",
    "Функция data_generator() будет генератором данных и будет принимать загруженные текстовые описания, признаки изображений, токенизатор и максимальную длину.8 ГБ оперативной памяти должно быть более чем достаточно.\n",
    "\n",
    "Вы можете видеть, что мы вызываем функцию create_sequence (), чтобы создать пакет данных для одного изображения, а не для всего набора данных. Это означает, что мы должны обновить функцию create_sequences (), чтобы удалить “итерацию по всем описаниям” для цикла.\n",
    "\n",
    "Генератор данных, предназначен для использования в вызове model.fit_generator().\n",
    "\n",
    "Обратите внимание, что это очень простой генератор данных. Большая экономия памяти, которую он предлагает, заключается в том, чтобы не иметь развернутых последовательностей обучающих и тестовых данных в памяти до подгонки модели, чтобы эти образцы (например, результаты create_sequences()) создавались по мере необходимости для каждого изображения.\n",
    "\n",
    "Некоторые нестандартные идеи для дальнейшего совершенствования этого генератора данных включают в себя:\n",
    "– Рандомизируйте порядок фотографий каждой эпохи.\n",
    "– Работайте со списком идентификаторов изображений и загружайте текст и данные изображений по мере необходимости, чтобы ещё больше сократить объём памяти.\n",
    "– Получите более одного изображения в партии."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Построение модели\n",
    "\n",
    "Мы опишем модель в трёх частях:\n",
    "\n",
    "1 – Извлечение признаков изображения. Это 16-слойная модель VGG, предварительно обученная на наборе данных ImageNet. Мы предварительно обработали изображения с помощью модели VGG (без выходного слоя) и будем использовать извлечённые признаки, предсказанные этой моделью, в качестве входных данных.\n",
    "\n",
    "2 – Обработка последовательностей. Это слой встраивания слов для обработки ввода текста, за которым следует слой рекуррентной нейронной сети с длительной кратковременной памятью (LSTM).\n",
    "\n",
    "3 – Расшифровка. (1) и (2) выводят вектор фиксированной длины. Они объединяются вместе и обрабатываются плотным слоем, чтобы сделать окончательный прогноз.\n",
    "\n",
    "Модель (1) ожидает, что входные признаки изображений будут вектором из 4096 элементов. Они обрабатываются плотным слоем для получения 256-элементного представления изображения.\n",
    "\n",
    "Модель (2) ожидает входные последовательности с заранее определённой длиной, которые подаются в слой встраивания, использующий маску для игнорирования дополненных значений. За этим следует слой LSTM с 256 единицами памяти.\n",
    "\n",
    "Обе входные модели создают вектор из 256 элементов. Кроме того, обе входные модели используют регуляризацию в виде 50% отсева (dropout). Это делается для того, чтобы уменьшить переобучение модели на текущем наборе данных, так как эта конфигурация модели обучается очень быстро.\n",
    "\n",
    "Модель (3) объединяет векторы из обеих входных моделей с помощью операции сложения. Затем этот вектор подаётся на плотный слой из 256 нейронов, а затем на конечный выходной плотный слой, который делает прогноз softmax по всему выходному словарю для следующего слова в последовательности."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Обучение\n",
    "\n",
    "В этом примере мы отбросим загрузку тестового набора данных и контрольные точки модели и просто сохраним модель после каждой эпохи обучения. Затем мы сможем вернуться и загрузить/оценить каждую сохраненную модель после обучения, чтобы найти ту, которая имеет наименьшие потери."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "кол-во подписей .............. 8262\n",
      "размер словаря предложений ... 21391\n",
      "длина подписи ................ 22\n"
     ]
    }
   ],
   "source": [
    "with open (path_train_dict, 'rb') as f:\n",
    "    train_dict = pickle.load(f)\n",
    "train_features = load_image_features(path_features, train_dict)\n",
    "print('кол-во подписей .............. %d' % len(train_dict))\n",
    "\n",
    "with open (path_sentences, 'rb') as f:\n",
    "    sentences = pickle.load(f)\n",
    "tokenizer = create_tokenizer(train_dict)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('размер словаря предложений ... %d' % vocab_size)\n",
    "\n",
    "max_length = find_max_words(train_dict)\n",
    "print('длина подписи ................ %d' % max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, max_length)\n",
    "epochs = 20\n",
    "batch_size = 16\n",
    "steps = len(train_dict)//batch_size\n",
    "if len(train_dict) % batch_size != 0:\n",
    "    steps = steps + 1\n",
    "\n",
    "#for i in range(epochs):\n",
    "generator = data_generator(tokenizer, max_length, train_dict, train_features, batch_size, 42)\n",
    "model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}