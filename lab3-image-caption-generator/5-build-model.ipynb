{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from tensorflow.python.client import device_lib\n",
    "from keras_preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.merge import add"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Расположение файлов"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "curr_folder = \"D:/YandexDisk/datasets/\"\n",
    "end_dir = \"D:/datasets/flickr-images-12k\"\n",
    "\n",
    "path_features = curr_folder + \"ru-12k-features.pkl\"\n",
    "path_vocab = curr_folder + \"ru-12k-vocab.pkl\"\n",
    "path_sentences = curr_folder + \"ru-12k-sentences-train.pkl\"\n",
    "\n",
    "path_train_dict = curr_folder + \"captions-ru-12k-train.pkl\"\n",
    "path_val_dict = curr_folder + \"captions-ru-12k-val.pkl\"\n",
    "\n",
    "path_model = curr_folder + 'model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Проверка видеокарты"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 4254697333940493407\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 3129068339\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 8106236309944312838\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загрузка данных\n",
    "\n",
    "Мы собираемся обучить данные по всем фотографиям и подписям в обучающем наборе данных. Во время обучения мы будем отслеживать производительность модели в наборе данных разработки и использовать эту производительность, чтобы решить, когда сохранять модели в файл.\n",
    "\n",
    "Модель, которую мы разработаем, будет генерировать подпись к фотографии, и подпись будет генерироваться по одному слову за раз.\n",
    "\n",
    "Последовательность ранее сгенерированных слов будет предоставлена в качестве входных данных. Поэтому нам понадобится \"первое слово\", чтобы начать процесс генерации, и \"последнее слово\", чтобы сигнализировать об окончании подписи. Для этой цели мы будем использовать строки \"startseq\" и \"endseq\". Эти маркеры добавляются к загруженным описаниям по мере их загрузки. Важно сделать это сейчас, прежде чем мы закодируем текст, чтобы токены также были закодированы правильно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def image_names_set(data):\n",
    "    vals = set()\n",
    "\n",
    "    for idx in data.index:\n",
    "        vals.add(data.iat[idx, 0][:-4])\n",
    "\n",
    "    return vals\n",
    "\n",
    "def load_image_features(filename, data):\n",
    "    all_features = pickle.load(open(filename, 'rb'))\n",
    "    features = {k: all_features[k] for k in data}\n",
    "\n",
    "    return features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Закодировать знаки в числа\n",
    "\n",
    "Текст описания необходимо будет закодировать в числа, прежде чем его можно будет представить модели в качестве входных данных или сравнить с предсказаниями модели.\n",
    "\n",
    "Первым шагом в кодировании данных является создание согласованного сопоставления слов с уникальными целочисленными значениями. Keras предоставляет класс Tokenizer, который может изучить это сопоставление из загруженных данных описания.\n",
    "\n",
    "Каждое описание будет разделено на слова. Модель будет предоставлена одним словом и фотографией и сгенерирует следующее слово. Затем первые два слова описания будут предоставлены модели в качестве входных данных вместе с изображением для создания следующего слова. Именно так будет обучаться модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def to_lines(data):\n",
    "    all_vals = list()\n",
    "    for key in data.keys():\n",
    "        [all_vals.append(d) for d in data[key]]\n",
    "\n",
    "    return all_vals\n",
    "\n",
    "def create_tokenizer(data):\n",
    "    lines = to_lines(data)\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "def find_max_words(data):\n",
    "    lines = to_lines(data)\n",
    "    return max(len(l.split()) for l in lines)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Создание последовательности\n",
    "\n",
    "Приведенная ниже функция с именем create_sequences(), учитывая токенизатор, максимальную длину последовательности и словарь всех описаний и фотографий, преобразует данные в пары ввода-вывода данных для обучения модели.\n",
    "\n",
    "В модели есть два входных массива: один для признаков фотографии и один для закодированного текста. Существует один вывод для модели, который представляет собой закодированное следующее слово в текстовой последовательности.\n",
    "\n",
    "Входной текст кодируется в виде целых чисел, которые будут передаваться на слой встраивания слов. Признаки изображения будут передаваться непосредственно в другую часть модели. Модель выдаст прогноз, который будет представлять собой распределение вероятностей по всем словам в словаре.\n",
    "\n",
    "Таким образом, выходные данные будут представлять собой однократно закодированную версию каждого слова, представляющую идеализированное распределение вероятностей со значениями 0 во всех позициях слов, кроме фактической позиции слова, которая имеет значение 1."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def create_sequences(tokenizer, max_words, captions_list, image_name):\n",
    "    X_image, X_text, y_word = list(), list(), list()\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "    for caption in captions_list:\n",
    "        seq = tokenizer.texts_to_sequences([caption])[0]\n",
    "\n",
    "        for i in range(1, len(seq)):\n",
    "            in_seq, out_seq = seq[:i], seq[i]\n",
    "            in_seq = pad_sequences([in_seq], maxlen=max_words)[0]\n",
    "            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "\n",
    "            X_image.append(image_name)\n",
    "            X_text.append(in_seq)\n",
    "            y_word.append(out_seq)\n",
    "\n",
    "    return X_image, X_text, y_word"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Генератор данных\n",
    "\n",
    "Генератор данных будет выдавать данные на одно изображении в каждой партии. Это будут все последовательности, сгенерированные для изображения и её набора описаний.\n",
    "\n",
    "Функция data_generator() будет генератором данных и будет принимать загруженные текстовые описания, признаки изображений, токенизатор и максимальную длину.8 ГБ оперативной памяти должно быть более чем достаточно.\n",
    "\n",
    "Вы можете видеть, что мы вызываем функцию create_sequence (), чтобы создать пакет данных для одного изображения, а не для всего набора данных. Это означает, что мы должны обновить функцию create_sequences (), чтобы удалить “итерацию по всем описаниям” для цикла.\n",
    "\n",
    "Генератор данных, предназначен для использования в вызове model.fit_generator().\n",
    "\n",
    "Обратите внимание, что это очень простой генератор данных. Большая экономия памяти, которую он предлагает, заключается в том, чтобы не иметь развернутых последовательностей обучающих и тестовых данных в памяти до подгонки модели, чтобы эти образцы (например, результаты create_sequences()) создавались по мере необходимости для каждого изображения.\n",
    "\n",
    "Некоторые нестандартные идеи для дальнейшего совершенствования этого генератора данных включают в себя:\n",
    "– Рандомизируйте порядок фотографий каждой эпохи.\n",
    "– Работайте со списком идентификаторов изображений и загружайте текст и данные изображений по мере необходимости, чтобы ещё больше сократить объём памяти.\n",
    "– Получите более одного изображения в партии."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def data_generator(tokenizer, max_words, data, images, batch_size, random_seed):\n",
    "    count = 0\n",
    "    random.seed(random_seed)\n",
    "\n",
    "    img_names = list(data.keys())\n",
    "    assert batch_size <= len(img_names), 'batch size must be less than or equal to {}'.format(len(img_names))\n",
    "\n",
    "    while True:\n",
    "        input_img_batch, input_seq_batch, output_word_batch = list(), list(), list()\n",
    "\n",
    "        if count >= len(img_names):\n",
    "            count = 0\n",
    "        start_i = count\n",
    "        end_i = min(len(img_names), count + batch_size)\n",
    "\n",
    "        for i in range(start_i, end_i):\n",
    "            curr_img = img_names[i]\n",
    "            image = images[curr_img][0]\n",
    "            captions_list = data[curr_img]\n",
    "            random.shuffle(captions_list)\n",
    "\n",
    "            input_img, input_seq, output_word = create_sequences(tokenizer, max_words, captions_list, image)\n",
    "\n",
    "            for j in range(len(input_img)):\n",
    "                input_img_batch.append(input_img[j])\n",
    "                input_seq_batch.append(input_seq[j])\n",
    "                output_word_batch.append(output_word[j])\n",
    "\n",
    "        count = count + batch_size\n",
    "        yield [np.array(input_img_batch), np.array(input_seq_batch)], np.array(output_word_batch)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Построение модели\n",
    "\n",
    "Мы опишем модель в трёх частях:\n",
    "\n",
    "1 – Извлечение признаков изображения. Это 16-слойная модель VGG, предварительно обученная на наборе данных ImageNet. Мы предварительно обработали изображения с помощью модели VGG (без выходного слоя) и будем использовать извлечённые признаки, предсказанные этой моделью, в качестве входных данных.\n",
    "\n",
    "2 – Обработка последовательностей. Это слой встраивания слов для обработки ввода текста, за которым следует слой рекуррентной нейронной сети с длительной кратковременной памятью (LSTM).\n",
    "\n",
    "3 – Расшифровка. (1) и (2) выводят вектор фиксированной длины. Они объединяются вместе и обрабатываются плотным слоем, чтобы сделать окончательный прогноз.\n",
    "\n",
    "Модель (1) ожидает, что входные признаки изображений будут вектором из 4096 элементов. Они обрабатываются плотным слоем для получения 256-элементного представления изображения.\n",
    "\n",
    "Модель (2) ожидает входные последовательности с заранее определённой длиной, которые подаются в слой встраивания, использующий маску для игнорирования дополненных значений. За этим следует слой LSTM с 256 единицами памяти.\n",
    "\n",
    "Обе входные модели создают вектор из 256 элементов. Кроме того, обе входные модели используют регуляризацию в виде 50% отсева (dropout). Это делается для того, чтобы уменьшить переобучение модели на текущем наборе данных, так как эта конфигурация модели обучается очень быстро.\n",
    "\n",
    "Модель (3) объединяет векторы из обеих входных моделей с помощью операции сложения. Затем этот вектор подаётся на плотный слой из 256 нейронов, а затем на конечный выходной плотный слой, который делает прогноз softmax по всему выходному словарю для следующего слова в последовательности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def build_model(vocab_size, max_words):\n",
    "    inputs1 = Input(shape=(4096,))\n",
    "    fe1 = Dropout(0.5)(inputs1)\n",
    "    fe2 = Dense(256, activation='relu')(fe1)\n",
    "\n",
    "    inputs2 = Input(shape=(max_words,))\n",
    "    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "    se2 = Dropout(0.5)(se1)\n",
    "    se3 = LSTM(256)(se2)\n",
    "\n",
    "    de1 = add([fe2, se3])\n",
    "    de2 = Dense(256, activation='relu')(de1)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(de2)\n",
    "\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Обучение\n",
    "\n",
    "В этом примере мы отбросим загрузку тестового набора данных и контрольные точки модели и просто сохраним модель после каждой эпохи обучения. Затем мы сможем вернуться и загрузить/оценить каждую сохраненную модель после обучения, чтобы найти ту, которая имеет наименьшие потери."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Набор для обучения"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "кол-во подписей .............. 8262\n",
      "размер словаря ............... 21391\n",
      "длина предложения в словах ... 22\n"
     ]
    }
   ],
   "source": [
    "with open (path_train_dict, 'rb') as f:\n",
    "    train_dict = pickle.load(f)\n",
    "train_features = load_image_features(path_features, train_dict)\n",
    "print('кол-во подписей .............. %d' % len(train_dict))\n",
    "\n",
    "with open (path_sentences, 'rb') as f:\n",
    "    sentences = pickle.load(f)\n",
    "tokenizer = create_tokenizer(train_dict)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('размер словаря ............... %d' % vocab_size)\n",
    "\n",
    "max_words = find_max_words(train_dict)\n",
    "print('длина предложения в словах ... %d' % max_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Набор для валидации"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "кол-во подписей .............. 2360\n",
      "размер словаря ............... 10450\n",
      "длина предложения в словах ... 21\n"
     ]
    }
   ],
   "source": [
    "with open (path_val_dict, 'rb') as f:\n",
    "    val_dict = pickle.load(f)\n",
    "val_features = load_image_features(path_features, val_dict)\n",
    "print('кол-во подписей .............. %d' % len(val_dict))\n",
    "\n",
    "val_tokenizer = create_tokenizer(val_dict)\n",
    "val_vocab_size = len(val_tokenizer.word_index) + 1\n",
    "print('размер словаря ............... %d' % val_vocab_size)\n",
    "\n",
    "val_max_words = find_max_words(val_dict)\n",
    "print('длина предложения в словах ... %d' % val_max_words)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Параметры"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "batch_size = 16\n",
    "\n",
    "#checkpoint = ModelCheckpoint(path_model, save_best_only=True,\n",
    "#                             monitor='val_loss', mode='min',\n",
    "#                             verbose=1)\n",
    "\n",
    "#early_stop = EarlyStopping(monitor=\"val_loss\", mode=\"min\",\n",
    "#                           patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "517/517 [==============================] - 262s 492ms/step - loss: 6.4099\n",
      "517/517 [==============================] - 264s 510ms/step - loss: 4.5705\n",
      "517/517 [==============================] - 266s 514ms/step - loss: 4.0379\n",
      "517/517 [==============================] - 259s 500ms/step - loss: 3.6417\n",
      "517/517 [==============================] - 260s 503ms/step - loss: 3.3489\n",
      "517/517 [==============================] - 259s 501ms/step - loss: 3.1285\n",
      "517/517 [==============================] - 260s 501ms/step - loss: 2.9569\n",
      "517/517 [==============================] - 260s 502ms/step - loss: 2.8283\n",
      "517/517 [==============================] - 268s 518ms/step - loss: 2.7060\n",
      "517/517 [==============================] - 264s 510ms/step - loss: 2.5998\n",
      "517/517 [==============================] - 264s 510ms/step - loss: 2.5105\n",
      "517/517 [==============================] - 264s 510ms/step - loss: 2.4246\n",
      "517/517 [==============================] - 266s 514ms/step - loss: 2.3502\n",
      "517/517 [==============================] - 267s 516ms/step - loss: 2.2828\n",
      "517/517 [==============================] - 274s 529ms/step - loss: 2.2206\n",
      "517/517 [==============================] - 270s 521ms/step - loss: 2.1654\n",
      "517/517 [==============================] - 271s 523ms/step - loss: 2.1190\n",
      "517/517 [==============================] - 272s 525ms/step - loss: 2.0800\n",
      "517/517 [==============================] - 269s 520ms/step - loss: 2.0417\n",
      "517/517 [==============================] - 271s 524ms/step - loss: 2.0093\n"
     ]
    }
   ],
   "source": [
    "model = build_model(vocab_size, max_words)\n",
    "steps = len(train_dict)/batch_size\n",
    "if len(train_dict) % batch_size != 0:\n",
    "    steps = steps + 1\n",
    "#val_steps = len(val_dict)/batch_size\n",
    "#if len(val_dict) % batch_size != 0:\n",
    "#    val_steps = val_steps + 1\n",
    "\n",
    "start_time = time.time()\n",
    "for i in range(epochs):\n",
    "    generator = data_generator(tokenizer, max_words, train_dict, train_features, batch_size, 42)\n",
    "    #val_generator = data_generator(val_tokenizer, val_max_words, val_dict, val_features, batch_size, 42)\n",
    "    model.fit(generator,\n",
    "              epochs=1, steps_per_epoch=steps,\n",
    "              #callbacks=[checkpoint, early_stop],\n",
    "              #validation_data=val_generator, validation_steps=val_steps,\n",
    "              verbose=1)\n",
    "    model.save('model_' + str(i) + '.h5')\n",
    "time_difference = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "время обучения в минутах ..... 88\n"
     ]
    }
   ],
   "source": [
    "minutes = time_difference/60\n",
    "print('время обучения в минутах ..... %d' % minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Оценка модели\n",
    "\n",
    "После того, как модель обучены, мы можем оценить качество её предсказаний в тестовом наборе данных.\n",
    "\n",
    "Мы оценим модель, создав описания для всех изображений в тестовом наборе данных и оценив эти прогнозы с помощью стандартной функции затрат.\n",
    "\n",
    "Во-первых, нам нужно иметь возможность сгенерировать описание изображения, используя обученную модель. Это включает в себя передачу маркера начального описания \"startseq\", генерацию одного слова, а затем рекурсивный вызов модели с сгенерированными словами в качестве входных данных до тех пор, пока не будет достигнут конец маркера последовательности \"endseq\" или не будет достигнута максимальная длина описания.\n",
    "\n",
    "Приведённая ниже функция реализует это поведение и генерирует текстовое описание с учётом обученной модели и заданного подготовленного изображения в качестве входных данных. Эта функция вызывает другую функцию map_int_to_word, чтобы отобразить целочисленное предсказание обратно в слово."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "def map_int_to_word(integer, tokenizer):\n",
    "    for word, idx in tokenizer.word_index.items():\n",
    "        if idx == integer:\n",
    "            return word\n",
    "\n",
    "    return None\n",
    "\n",
    "# 0 seed the generation process\n",
    "# 1 iterate over the whole length of the sequence\n",
    "# 2 integer encode input sequence\n",
    "# 3 pad input\n",
    "# 4 predict next word\n",
    "# 5 convert probability to integer\n",
    "# 6 map integer to word\n",
    "# 7 stop if we cannot map the word\n",
    "# 8 append as input for generating the next word\n",
    "# 9 stop if we predict the end of the sequence\n",
    "def generate_caption(model, tokenizer, image, max_words):\n",
    "    in_text = 'startseq'                                    # 0\n",
    "\n",
    "    for i in range(max_words):                              # 1\n",
    "        seq = tokenizer.texts_to_sequences([in_text])[0]    # 2\n",
    "        seq = pad_sequences([seq], maxlen=max_words)        # 3\n",
    "\n",
    "        y_hat = model.predict([image,seq], verbose=0)       # 4\n",
    "        y_hat = np.argmax(y_hat)                            # 5\n",
    "\n",
    "        word = map_int_to_word(y_hat, tokenizer)            # 6\n",
    "        if word is None:                                    # 7\n",
    "            break\n",
    "\n",
    "        in_text += ' ' + word                               # 8\n",
    "\n",
    "        if word == 'endseq':                                # 9\n",
    "            break\n",
    "\n",
    "    return in_text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### BLEU\n",
    "\n",
    "Мы сгенерируем предсказания для всех изображений в тестовом наборе данных и в наборе данных для обучения.\n",
    "\n",
    "Приведенная ниже функция с именем evaluate_model() будет оценивать обученную модель по заданному набору подписей изображений и признаков изображений. Фактические и прогнозируемые описания собираются и оцениваются вместе с использованием оценки алгоритма BLEU, который оценивает, насколько сгенерированный текст близок к ожидаемому тексту.\n",
    "\n",
    "BLEU используется при переводе текста для оценки переведённого текста по одному или нескольким другим переводам.\n",
    "\n",
    "Here, we compare each generated description against all of the reference descriptions for the photograph. We then calculate BLEU scores for 1, 2, 3 and 4 cumulative n-grams.\n",
    "\n",
    "The NLTK Python library implements the BLEU score calculation in the corpus_bleu() function. A higher score close to 1.0 is better, a score closer to zero is worse."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def evaluate_model(model, captions, images, tokenizer, max_words):\n",
    "    actual, predicted = list(), list()\n",
    "\n",
    "    for key, captions_list in captions.items():\n",
    "        references = [c.split() for c in captions_list]\n",
    "        y_hat = generate_caption(model, tokenizer, images[key], max_words)\n",
    "\n",
    "        actual.append(references)\n",
    "        predicted.append(y_hat.split())\n",
    "\n",
    "    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "    print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "    print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
    "    print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}